{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhnM+QtgamAghofehm31Kr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/audrb1999/DL-Project/blob/main/01.numpy_XOR_%EA%B5%AC%ED%98%84/%EC%B5%9C%EC%A2%85%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz8jQuIVWo8a"
      },
      "source": [
        "# 딥러닝 기본기 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPtvLLJlsjWr"
      },
      "source": [
        "##***기본적인 클래스 구현***\n",
        "1. Layer\n",
        "2. Step function\n",
        "3. Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWB6prAZoKdC"
      },
      "source": [
        "class Layer:\n",
        "    def __init__(self):# 모든 함수들이 공통으로 입출력을 조절\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "    \n",
        "    def forward(self, x):# 모든 함수들이 공통으로 사용하는 순전파 \n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwYNZ92foQvg"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class StepFunction(Layer):\n",
        "    def _forward(self, x):\n",
        "        y = x > 0\n",
        "        return y.astype(np.int)\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        a = np.matmul(x, self.weights) + self.bias\n",
        "        if self.activation != None:\n",
        "            y = self.activation.forward(a)\n",
        "            return y\n",
        "        return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pxpwv0WtWep"
      },
      "source": [
        "\n",
        "1. 퍼셉트론이 제대로 동작하는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6c5MTAoq42O",
        "outputId": "98f4e441-4cfe-4c45-bc40-a9619bcd10a0"
      },
      "source": [
        "p = Perceptron(np.array([[1, 2], [3, 4]]), np.array([[5, 6]]))\n",
        "p.forward(np.array([[0, 0], [1, 0], [0, 1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  6],\n",
              "       [ 6,  8],\n",
              "       [ 8, 10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWa9cEhhufoZ"
      },
      "source": [
        "2. 계단 함수가 제대로 동작하는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGliaM-PrUBX",
        "outputId": "d44877aa-50da-4f13-dbab-2d780ef554de"
      },
      "source": [
        "s = StepFunction()\n",
        "s.forward(np.array([1, -4, 3, 0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUtJp6AHun96"
      },
      "source": [
        "3. 퍼셉트론과 계단함수를 이용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1N2ie7Xsg_w",
        "outputId": "31e163ef-aeda-4728-af48-4a065684a684"
      },
      "source": [
        "p = Perceptron(np.array([[1, -1], [-2, 0]]), np.array([[-3, 1]]), activation=s)\n",
        "p.forward(np.array([[1, 2], [-3, -4]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUmk8X3MviXp"
      },
      "source": [
        "##***MLP를 위한 Class 만들기***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GQAu9ohvL2I"
      },
      "source": [
        "class Model(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        a = self.layers[0].forward(x)\n",
        "        y = self.layers[1].forward(a)\n",
        "\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIf1R4-HwlUx",
        "outputId": "168b8da3-8c73-47d1-ec12-0c2e90108bc0"
      },
      "source": [
        "model = Model()\n",
        "\n",
        "model.add(Perceptron(np.array([[1, 2], [1, 0]]), np.array([[-1, -2]])))\n",
        "model.add(Perceptron(np.array([[-1, 1], [3, 1]]), np.array([[0, 1]])))\n",
        "model.forward(np.array([[1, 1], [0, 0]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1,  2],\n",
              "       [-5, -2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnqBMrKNyQni"
      },
      "source": [
        "##And 게이트를 구현해보자 # 가중치만 건드려주면 된다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvDTdFEawmn3",
        "outputId": "8bb6b1c8-8523-4038-e87d-091c37d68041"
      },
      "source": [
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        a = np.matmul(x, self.weights) + self.bias\n",
        "        if self.activation != None:\n",
        "            y = self.activation.forward(a)\n",
        "            return y\n",
        "        return a\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        y = self.layers[0].forward(x)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "s = StepFunction()\n",
        "\n",
        "model = Model()# 초기화\n",
        "model.add(Perceptron(np.array([0.3, 0.3]), -0.5, s))\n",
        "model.forward(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIv72t5h50C8"
      },
      "source": [
        "##***XOR 문제를 해결해보자***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIcPkwZHywCv"
      },
      "source": [
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        a = np.matmul(x, self.weights) + self.bias\n",
        "        if self.activation != None:\n",
        "            y = self.activation.forward(a)\n",
        "            return y\n",
        "        return a\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        a = self.layers[0].forward(x)# 2층으로 연결해주기 위해서 이 출력을 다음 레이어가 받음\n",
        "        c = self.layers[1].forward(x)\n",
        "        y = []\n",
        "        print('a값', len(a))# 4번 반복을 위해\n",
        "        for i in range(len(a)):    \n",
        "            z = Perceptron(np.array([0.3, 0.3]), -0.5, StepFunction()).forward(np.array([a[i], c[i]]))\n",
        "            y.append(z)\n",
        "            \n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_aJBesZ3U60",
        "outputId": "b68cd282-6efe-427a-e46d-c7f6671bd25d"
      },
      "source": [
        "s = StepFunction()\n",
        "model = Model()\n",
        "model.add(Perceptron(np.array([0.3, 0.3]), -0.2, activation = s))# OR\n",
        "model.add(Perceptron(np.array([-0.5, -0.5]), 0.8, s))# nand\n",
        "model.forward(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a값 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 1, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li9SvY5qD6H9"
      },
      "source": [
        "##***Sigmoid를 만들어보자!***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xwgZ1IdD9dl"
      },
      "source": [
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        return 1 / (1+np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB1X888FEsfS",
        "outputId": "978323a1-cddf-451a-c7b2-5dfeaad704eb"
      },
      "source": [
        ">>> s = Sigmoid()\n",
        ">>> s.forward(np.array([-2, -1, 0, 1, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.11920292, 0.26894142, 0.5       , 0.73105858, 0.88079708])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmTP73tcE3H2"
      },
      "source": [
        "##***MSE(Loss function)을 만들어보자***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDiinUvnE-5L"
      },
      "source": [
        "class Loss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "   \n",
        "    def _forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class MSE(Loss):\n",
        "    def forward(self, y_true, y_pred):# forward를 상속받아서 고쳐쓴 것\n",
        "        mse = np.mean(np.square(y_true - y_pred)/2, axis = 0, keepdims = True)# 1랭크가 되지 않도록 차원 유지!\n",
        "        return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbWT3RBpGQCY",
        "outputId": "17c5591c-943d-4395-c06d-0c23dee08e72"
      },
      "source": [
        "mse = MSE()\n",
        "mse.forward(np.array([1, 2, 3, 4]), np.array([0, 0, 0, 0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuNhob_NHK4w"
      },
      "source": [
        "##***Sigmoid를 사용해서 xor을 구하고 loss값을 구하자!***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChF-EmoDGUV2"
      },
      "source": [
        "class Model(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.loss = None #loss값 초기화 이전 값이 있더라도 model()클래스 선언시 초기화됨!\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        a = self.layers[0].forward(x)# 2층으로 연결해주기 위해서 이 출력을 다음 레이어가 받음\n",
        "        c = self.layers[1].forward(x)\n",
        "        y = []\n",
        "        for i in range(len(a)):    \n",
        "            z = Perceptron(np.array([0.3, 0.3]), 0.1, StepFunction()).forward(np.array([a[i], c[i]]))\n",
        "            y.append(z)\n",
        "            \n",
        "        return y\n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        y_pred = Sigmoid().forward(x)\n",
        "        \n",
        "        return self.loss.forward(y, y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh95eeHdK-sT",
        "outputId": "7c5eba8a-d786-4a4a-daa7-549ece626141"
      },
      "source": [
        "model = Model()\n",
        "model.set_loss(MSE())\n",
        "model.evaluate(np.random.normal(size=(4, 2)), np.ones((4, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2228071680105223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMJCYMySstKW"
      },
      "source": [
        "##***Layer sigmoid역전파 구현!***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7KTURQvssjI"
      },
      "source": [
        "class Layer:\n",
        "    def backward(self, grad = 1):\n",
        "        if self.input is None or self.output is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        self.result = 1 / (1+np.exp(-x))\n",
        "        return self.result\n",
        "\n",
        "    def _backward(self, x): \n",
        "        result = self.result\n",
        "        return result * (1 - result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT-w3hwiXspc"
      },
      "source": [
        "##***퍼셉트론에 backward구현***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDJNE9oKt1Cf"
      },
      "source": [
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        \n",
        "        a = np.matmul(x, self.weights) + self.bias\n",
        "        if self.activation != None:\n",
        "            y = self.activation.forward(a)\n",
        "            return y\n",
        "        return a\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        #grad 값이 없으면 grad = 1\n",
        "        dw = self.input.T \n",
        "        db = np.sum(dz, axis = 0, keepdims=True)\n",
        "\n",
        "        return dw, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G52Nd4Kt4l7"
      },
      "source": [
        "##***시그모이드 안쓰고 구현***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN_5ahCKLAWE"
      },
      "source": [
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        self.result = 1 / (1+np.exp(-x))\n",
        "        return self.result\n",
        "\n",
        "    def _backward(self): \n",
        "        result = self.result\n",
        "        return result * (1 - result)\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        \n",
        "        a = np.matmul(x, self.weights) + self.bias\n",
        "        if self.activation != None:\n",
        "            y = self.activation.forward(a)\n",
        "            return y\n",
        "        return a\n",
        "\n",
        "    def _backward(self):\n",
        "        da = self.activation.backward() \n",
        "        dw = self.input.T @ dz\n",
        "        db = np.sum(dz, axis = 0, keepdims=True)\n",
        "\n",
        "        return dw, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8WzytUZgbT",
        "outputId": "e99f59da-24d9-4ee4-e9d5-2554432a8358"
      },
      "source": [
        "p = Perceptron(np.array([[1, 2], [3, -1]]), np.array([[-2, 0]]), activation=Sigmoid())\n",
        "x = np.arange(14).reshape((7, 2)) / 10\n",
        "p.forward(x)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15446527, 0.47502081],\n",
              "       [0.2890505 , 0.52497919],\n",
              "       [0.47502081, 0.57444252],\n",
              "       [0.66818777, 0.62245933],\n",
              "       [0.81757448, 0.66818777],\n",
              "       [0.90887704, 0.7109495 ],\n",
              "       [0.95689275, 0.75026011]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "rbZIq2ZnZhwK",
        "outputId": "521f14eb-8c23-4513-a18f-559b955287fa"
      },
      "source": [
        "print(p.backward())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-07550233c559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-b0f81a8dab6d>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _backward() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLOdpVqaZzpJ",
        "outputId": "156213df-112c-47ec-f1ab-09c8a3478c7c"
      },
      "source": [
        "print(g_w)\n",
        "print()\n",
        "print(g_b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.52551375 0.89637522]\n",
            " [0.63355475 1.05565494]]\n",
            "\n",
            "[[1.08041001 1.59279716]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JugmLWa3cV7S"
      },
      "source": [
        "##***Loss function 구현 역전파***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGr7LB9ab0Vu"
      },
      "source": [
        "class Loss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "   \n",
        "    def _forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, y_ture, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class MSE(Loss):\n",
        "    def forward(self, y_true, y_pred):# forward를 상속받아서 고쳐쓴 것\n",
        "        mse = np.mean(np.square(y_true - y_pred)/2, axis = 0, keepdims = True)# 1랭크가 되지 않도록 차원 유지!\n",
        "        return mse\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        grad = np.mean(y_pred - y_true, axis = 0, keepdims=True)# 평균, 차원 유지\n",
        "        return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO_DczxPdmWi",
        "outputId": "d78773e5-a239-482f-d113-b7809001168b"
      },
      "source": [
        "mse = MSE()\n",
        "mse.backward(np.array([1., 2., 3., 4.]), np.array([0., 0., 0., 0.]))#임의의 예측값과 답을 넣음"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48-jzNhAduMD",
        "outputId": "b956e57a-f2b3-43ff-9ecc-8728809c2761"
      },
      "source": [
        "mse.backward(np.array([[1., 2.], [3., 4.]]), np.array([[0., 0.], [0., 0.]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2., -3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnuYrqXRd-v-",
        "outputId": "71142220-8474-4191-d231-8d2783eb5fd6"
      },
      "source": [
        "k = mse.backward(np.random.normal(size=(3, 8, 8)), np.random.normal(size=(3, 8, 8)))\n",
        "k.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArM7kOomeak"
      },
      "source": [
        "##. Back propgation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_SgBwq_wKMi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):# 모든 함수들이 공통으로 입출력을 조절\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        \n",
        "    \n",
        "    def forward(self, x):# 모든 함수들이 공통으로 사용하는 순전파 \n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self, grad = 1):\n",
        "        if self.input is None or self.output is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def _backward(self,z): \n",
        "        return z * (1 - z)\n",
        "\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.z = None\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        self.z = x @ self.weights + self.bias\n",
        "        if self.activation != None:\n",
        "            self.a = self.activation.forward(self.z)\n",
        "            return self.a\n",
        "        return self.z\n",
        "\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.loss = None #loss값 초기화 이전 값이 있더라도 model()클래스 선언시 초기화됨!\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "\n",
        "        self.a1 = self.layers[0].forward(x)# 2층으로 연결해주기 위해서 이 출력을 다음 레이어가 받음\n",
        "        self.a2 = self.layers[1].forward(self.a1)\n",
        "            \n",
        "        return self.a2\n",
        "\n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        y_pred = Sigmoid().forward(x)\n",
        "        \n",
        "        return self.loss.forward(y, y_pred)\n",
        "\n",
        "    \n",
        "    def _backward(self, grad): # a2 = y, a1 = dz \n",
        "        \n",
        "        self.grad = grad # dl / da2\n",
        "        # dl/da2 * da2/dz2(시그모이드 역전파(self.layers[1].a))\n",
        "        # dl/da2는 axis = 0으로 sum 해주어서 브로드캐스팅을 사용하기 위해 일반 곱을 사용\n",
        "        self.dz2 = self.grad * self.layers[1].activation.backward(self.layers[1].a)# dl/da2 * da2/dz2 (4, 2)\n",
        "        self.db2 = np.sum(self.dz2, axis = 0, keepdims= True)# (dz2/db2)1 * dl/dz2 덧셈 노드라 그냥 전달(1, 2)\n",
        "        self.dw2 = self.layers[0].a.T @ self.dz2 # a1으로 w2 값 구하고  dz2/dw2 * da2/dz2 (5, 4) (4, 2) = (5, 2)\n",
        "        \n",
        "\n",
        "        # dl/dz2 * dz2/da1 * da1/dz1   \n",
        "        self.dz1 =  self.layers[0].activation.backward(self.dz2 @ self.layers[1].weights.T)  #(4, 2) (2, 5) (4, 5) 행렬곱으로는 맞춰줄 수 없어 방법 변경\n",
        "        self.dw1 = self.input.T @ self.dz1 # (3, 4)\n",
        "        self.db1 = np.sum(self.dz1, axis = 0, keepdims= True)\n",
        "       \n",
        "        return [[self.db1, self.db2], [self.dw1, self.dw2]]\n",
        "\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "   \n",
        "    def _forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, y_ture, y_pred):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "\n",
        "class MSE(Loss):\n",
        "    def forward(self, y_true, y_pred):# forward를 상속받아서 고쳐쓴 것\n",
        "        mse = np.mean(np.square(y_true - y_pred)/2, axis = 0, keepdims = True)# 1랭크가 되지 않도록 차원 유지!\n",
        "        return mse\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        grad = np.mean(y_true - y_pred, axis = 0, keepdims=True)# 평균, 차원 유지\n",
        "        return grad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsuyeWXep16D",
        "outputId": "1deda48f-7f0a-43c5-cc3a-180701d7b91e"
      },
      "source": [
        "_# 2 층 순전파\n",
        "model = Model()\n",
        "model.add(Perceptron(np.random.normal(size=(3, 5)), np.random.normal(size=(1, 5)), activation=Sigmoid()))\n",
        "model.add(Perceptron(np.random.normal(size=(5, 2)), np.random.normal(size=(1, 2)), activation=Sigmoid()))\n",
        "model.forward(np.ones((4, 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.37599948, 0.33568898],\n",
              "       [0.37599948, 0.33568898],\n",
              "       [0.37599948, 0.33568898],\n",
              "       [0.37599948, 0.33568898]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixtTPIrfDYPC",
        "outputId": "e14f15db-6adb-4880-8010-f029132eba30"
      },
      "source": [
        "grads = model.backward()\n",
        "\n",
        "b, w = grads\n",
        "\n",
        "print(w[0].shape, w[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 5) (5, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9QVWlEm-S1d",
        "outputId": "e9eeb4ee-5820-4bee-9de9-8e00c7b2dd4d"
      },
      "source": [
        "model.layers[1].a.T "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.84656494, 0.84656494, 0.84656494, 0.84656494],\n",
              "       [0.77027398, 0.77027398, 0.77027398, 0.77027398]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2GZLBSay_dG"
      },
      "source": [
        "## 가중치 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWFRequSzzSt"
      },
      "source": [
        "class Layer:\n",
        "    def __init__(self):# 모든 함수들이 공통으로 입출력을 조절\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        \n",
        "    \n",
        "    def forward(self, x):# 모든 함수들이 공통으로 사용하는 순전파 \n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self, grad = 1):\n",
        "        if self.input is None or self.output is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def _backward(self,z): \n",
        "        return z * (1 - z)\n",
        "\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.z = None\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        self.z = x @ self.weights + self.bias\n",
        "        if self.activation != None:\n",
        "            self.a = self.activation.forward(self.z)\n",
        "            return self.a\n",
        "        return self.z\n",
        "\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self, lr = 1e-3):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.loss = None #loss값 초기화 이전 값이 있더라도 model()클래스 선언시 초기화됨!\n",
        "        self.lr = lr\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "\n",
        "        self.a1 = self.layers[0].forward(x)# 2층으로 연결해주기 위해서 이 출력을 다음 레이어가 받음\n",
        "        self.a2 = self.layers[1].forward(self.a1)\n",
        "            \n",
        "        return self.a2\n",
        "\n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        y_pred = Sigmoid().forward(x)\n",
        "        \n",
        "        return self.loss.forward(y, y_pred)\n",
        "\n",
        "\n",
        "    def _backward(self, grad): # a2 = y, a1 = dz \n",
        "        \n",
        "        self.grad = grad # dl / da2\n",
        "        # dl/da2 * da2/dz2(시그모이드 역전파(self.layers[1].a))\n",
        "        # dl/da2는 axis = 0으로 sum 해주어서 브로드캐스팅을 사용하기 위해 일반 곱을 사용\n",
        "        self.dz2 = self.grad * self.layers[1].activation.backward(self.layers[1].a)# dl/da2 * da2/dz2 (4, 2)\n",
        "        self.db2 = np.sum(self.dz2, axis = 0, keepdims= True)# (dz2/db2)1 * dl/dz2 덧셈 노드라 그냥 전달(1, 2)\n",
        "        self.dw2 = self.layers[0].a.T @ self.dz2 # a1으로 w2 값 구하고  dz2/dw2 * da2/dz2 (5, 4) (4, 2) = (5, 2)\n",
        "        \n",
        "\n",
        "        # dl/dz2 * dz2/da1 * da1/dz1   \n",
        "        self.dz1 =  self.layers[0].activation.backward(self.dz2 @ self.layers[1].weights.T)  #(4, 2) (2, 5) (4, 5)\n",
        "        self.dw1 = self.input.T @ self.dz1 # (3, 4)\n",
        "        self.db1 = np.sum(self.dz1, axis = 0, keepdims= True)\n",
        "       \n",
        "        return [[self.db1, self.db2], [self.dw1, self.dw2]]\n",
        "\n",
        "    def update(self, grads):\n",
        "        \n",
        "        b, w = grads\n",
        "\n",
        "        self.layers[0].weights -= w[0] * self.lr\n",
        "        self.layers[1].weights -= w[1] * self.lr\n",
        "        self.layers[0].bias -= b[0] + self.lr\n",
        "        self.layers[1].bias -= b[1] * self.lr\n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVS345Xjb8H",
        "outputId": "b78a3652-3806-47c6-d583-4f01aeb21530"
      },
      "source": [
        "model = Model()\n",
        "model.add(Perceptron(np.random.normal(size=(3, 5)), np.random.normal(size=(1, 5)), activation=Sigmoid()))\n",
        "model.add(Perceptron(np.random.normal(size=(5, 2)), np.random.normal(size=(1, 2)), activation=Sigmoid()))\n",
        "model.lr = 1e-3\n",
        "model.forward(np.random.normal(size=(4, 3)))\n",
        "grads = model.backward()\n",
        "# 갱신 전\n",
        "print(model.layers[0].weights)\n",
        "print(model.layers[1].weights)\n",
        "print(model.layers[0].bias )\n",
        "print(model.layers[1].bias)\n",
        "\n",
        "model.update(grads)\n",
        "\n",
        "\n",
        "# 갱신 후\n",
        "print('갱신 후')\n",
        "print(model.layers[0].weights)\n",
        "print(model.layers[1].weights)\n",
        "print(model.layers[0].bias )\n",
        "print(model.layers[1].bias)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.80444546 -0.09864425  1.22167714 -1.84819749  0.13947525]\n",
            " [-0.46925474  0.13131593  0.01688424  1.31549757  0.31561143]\n",
            " [ 1.27558382 -0.08471281  0.52111704  0.59170133  1.12217941]]\n",
            "[[-1.38177342  0.04013651]\n",
            " [ 0.06273464 -1.16851579]\n",
            " [ 1.343578   -3.22086696]\n",
            " [ 0.08159677 -1.43991148]\n",
            " [-0.66887452  0.82060609]]\n",
            "[[-0.7886963  -0.45249229  0.261665    0.43971072  1.02026515]]\n",
            "[[-0.37223273  1.0588857 ]]\n",
            "갱신 후\n",
            "[[-0.80526485 -0.09921815  1.22066411 -1.84893067  0.13951008]\n",
            " [-0.4690221   0.13129008  0.01658335  1.31545817  0.31569122]\n",
            " [ 1.27520083 -0.08503966  0.520487    0.59128133  1.12223287]]\n",
            "[[-1.38210193  0.03986673]\n",
            " [ 0.06237703 -1.16881068]\n",
            " [ 1.34324211 -3.2211255 ]\n",
            " [ 0.08096765 -1.44040337]\n",
            " [-0.66942663  0.82016544]]\n",
            "[[0.7249716  0.49554672 1.8219941  1.64981106 1.01674288]]\n",
            "[[-0.37310033  1.05816795]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j__pc1iaHiXq"
      },
      "source": [
        "## Model 트레이닝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meof9XyUHlnp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):# 모든 함수들이 공통으로 입출력을 조절\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        \n",
        "    \n",
        "    def forward(self, x):# 모든 함수들이 공통으로 사용하는 순전파 \n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self, grad = 1):\n",
        "        if self.input is None or self.output is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.z = None\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        self.z = x @ self.weights + self.bias\n",
        "        if self.activation != None:\n",
        "            self.a = self.activation.forward(self.z)\n",
        "            return self.a\n",
        "        return self.z\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def _backward(self,z): \n",
        "        return z * (1 - z)\n",
        "\n",
        "class Loss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "   \n",
        "    def _forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, y_ture, y_pred):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "\n",
        "class MSE(Loss):\n",
        "    def forward(self, y_true, y_pred):# forward를 상속받아서 고쳐쓴 것\n",
        "        mse = 0.5 * np.mean(np.square(y_true - y_pred) , axis = 0, keepdims = True)# 1랭크가 되지 않도록 차원 유지!\n",
        "        return mse\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        grad = 2 * np.mean(y_pred - y_true, axis = 0, keepdims=True)# 평균, 차원 유지\n",
        "        return grad\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def _backward(self,z): \n",
        "        return z * (1 - z)\n",
        "\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self, lr = 1e-3):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.loss = None #loss값 초기화 이전 값이 있더라도 model()클래스 선언시 초기화됨!\n",
        "        self.lr = lr\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "\n",
        "        self.a1 = self.layers[0].forward(x)# 2층으로 연결해주기 위해서 이 출력을 다음 레이어가 받음\n",
        "        self.a2 = self.layers[1].forward(self.a1)\n",
        "            \n",
        "        return self.a2\n",
        "\n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        y_pred = Sigmoid().forward(x)\n",
        "        \n",
        "        return self.loss.forward(y, y_pred)\n",
        "\n",
        "    \n",
        "    def _backward(self, grad): # a2 = y, a1 = dz \n",
        "        \n",
        "        self.grad = grad # dl / da2\n",
        "        # dl/da2 * da2/dz2(시그모이드 역전파(self.layers[1].a))\n",
        "        # dl/da2는 axis = 0으로 sum 해주어서 브로드캐스팅을 사용하기 위해 일반 곱을 사용\n",
        "        self.dz2 = self.grad * self.layers[1].activation.backward(self.layers[1].a)# dl/da2 * da2/dz2 (4, 2)\n",
        "        self.db2 = np.sum(self.dz2, axis = 0, keepdims= True)# (dz2/db2)1 * dl/dz2 덧셈 노드라 그냥 전달(1, 2)\n",
        "        self.dw2 = self.layers[0].a.T @ self.dz2 # a1으로 w2 값 구하고  dz2/dw2 * da2/dz2 (5, 4) (4, 2) = (5, 2)\n",
        "        \n",
        "\n",
        "        # dl/dz2 * dz2/da1 * da1/dz1   \n",
        "        self.dz1 =  self.layers[0].activation.backward(self.dz2 @ self.layers[1].weights.T)  #(4, 2) (2, 5) (4, 5)\n",
        "        self.dw1 = self.input.T @ self.dz1 # (3, 4)\n",
        "        self.db1 = np.sum(self.dz1, axis = 0, keepdims= True)\n",
        "       \n",
        "        return [[self.db1, self.db2], [self.dw1, self.dw2]]\n",
        "        \n",
        "        b, w = grads\n",
        "\n",
        "        self.layers[0].weights -= w[0] * self.lr\n",
        "        self.layers[1].weights -= w[1] * self.lr\n",
        "        self.layers[0].bias -= b[0] + self.lr\n",
        "        self.layers[1].bias -= b[1] * self.lr\n",
        "\n",
        "\n",
        "    def update(self, grads):\n",
        "        \n",
        "        b, w = grads\n",
        "\n",
        "        self.layers[0].weights -= w[0] * self.lr\n",
        "        self.layers[1].weights -= w[1] * self.lr\n",
        "        self.layers[0].bias -= b[0] + self.lr\n",
        "        self.layers[1].bias -= b[1] * self.lr\n",
        "\n",
        "\n",
        "    def train_once(self, x, y):\n",
        "        y_pred = self.forward(x)\n",
        "        loss = self.loss.backward(y, y_pred)\n",
        "        grads = self.backward()\n",
        "        self.update(grads)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADZakQwQJ1Q_"
      },
      "source": [
        ">>> model = Model()\n",
        ">>> model.add(Perceptron(np.random.normal(size=(3, 5)), np.random.normal(size=(1, 5)), activation=Sigmoid()))\n",
        ">>> model.add(Perceptron(np.random.normal(size=(5, 3)), np.random.normal(size=(1, 3)), activation=Sigmoid()))\n",
        ">>> model.set_loss(MSE())\n",
        ">>> model.lr = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfnjpon7JRS5",
        "outputId": "7cd1b2cc-13c2-4364-be08-386901fd7a09"
      },
      "source": [
        "for i in range(10000):\n",
        "    x = np.random.normal(size=(4, 3))\n",
        "    y = -0.5 * x\n",
        "    model.train_once(x, y)\n",
        "\n",
        "x = np.random.normal(size=(10, 3))\n",
        "y = -0.5 * x\n",
        "model.evaluate(x, y)\n",
        "\n",
        "\"\"\"    def update(self, grads):\n",
        "        \n",
        "        b, w = grads\n",
        "\n",
        "        self.layers[0].weights -= w[0] * self.lr\n",
        "        self.layers[1].weights -= w[1] * self.lr\n",
        "        self.layers[0].bias -= b[0] + self.lr\n",
        "        self.layers[1].bias -= b[1] * self.lr\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00000000e+000 6.26310965e-118 1.00000000e+000 1.00000000e+000\n",
            "  1.00000000e+000]\n",
            " [1.00000000e+000 7.19037260e-117 1.00000000e+000 1.00000000e+000\n",
            "  1.00000000e+000]\n",
            " [1.00000000e+000 7.82750444e-117 1.00000000e+000 1.00000000e+000\n",
            "  1.00000000e+000]\n",
            " [1.00000000e+000 3.08829742e-117 1.00000000e+000 1.00000000e+000\n",
            "  1.00000000e+000]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdGyTi1IjKDq",
        "outputId": "93fde81c-3418-4be1-fa32-3a406dfad661"
      },
      "source": [
        "for i in range(10000):\n",
        "    x = np.random.normal(size=(10, 3))\n",
        "    y = -0.5 * x\n",
        "    model.train_once(x, y)\n",
        "\n",
        "x = np.random.normal(size=(10, 3))\n",
        "y = -0.5 * x\n",
        "model.evaluate(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25809517, 0.07422035, 0.26211177]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm0ldlk0m315"
      },
      "source": [
        "## 실제 xor 학습을 해보자 - 1차 시도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5O2otRRm7YZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):# 모든 함수들이 공통으로 입출력을 조절\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        \n",
        "    \n",
        "    def forward(self, x):# 모든 함수들이 공통으로 사용하는 순전파 \n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self, grad = 1):\n",
        "        if self.input is None or self.output is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__() # 만약 상위 클래스 Layer가 호출되지 않아도 실행할 수 있게하는 것\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "    def _forward(self, x): # 활성화 함수 유무를 위한 if 문\n",
        "        self.z = x @ self.weights + self.bias\n",
        "        if self.activation != None:\n",
        "            self.a = self.activation.forward(self.z)\n",
        "            return self.a\n",
        "        return self.z\n",
        "\n",
        "class Loss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "   \n",
        "    def _forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, y_ture, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "        \n",
        "\n",
        "class MSE(Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_true, y_pred):# forward를 상속받아서 고쳐쓴 것\n",
        "        self.mse = 0.5 * np.mean(np.square(y_pred - y_true) , axis = 0, keepdims = True)# 1랭크가 되지 않도록 차원 유지!\n",
        "        return self.mse\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        self.grad =  np.mean(y_pred - y_true, axis = 0, keepdims=True)# 평균, 차원 유지\n",
        "        return self.grad\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def _forward(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def _backward(self,z): \n",
        "        return z * (1 - z)\n",
        "\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self, lr = 0.001):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.loss = None #loss값 초기화 이전 값이 있더라도 model()클래스 선언시 초기화됨!\n",
        "        self.lr = lr\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "   \n",
        "        self.a1 = self.layers[0].forward(x)# 2층으로 연결해주기 위해서 이 출력을 다음 레이어가 받음\n",
        "        self.a2 = self.layers[1].forward(self.a1)\n",
        "            \n",
        "        return self.a2\n",
        "        \n",
        "    def set_node(self):\n",
        "        self.W1 = self.layers[0].weight\n",
        "        self.b1 = self.layers[0].bias\n",
        "        self.layers[0].weight = self.W1\n",
        "        self.layers[0].bias = self.b1\n",
        "        self.z1 = np.dot(self.x,self.W1)+self.b1\n",
        "        self.s1 = self.layers[0].activation.forward(self.z1)\n",
        "\n",
        "\n",
        "        self.W2 = self.layers[1].weight\n",
        "        self.b2 = self.layers[1].bias\n",
        "        self.layers[1].weight = self.W2\n",
        "        self.layers[1].bias = self.b2\n",
        "        self.z2 = np.dot(self.s1,self.W2)+self.b2\n",
        "        self.s2 = self.layers[1].activation.forward(self.z2)\n",
        "\n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        y_pred = Sigmoid().forward(x)\n",
        "        \n",
        "        return self.loss.forward(y, y_pred)\n",
        "\n",
        "    \n",
        "    def _backward(self, grad): # a2 = y, a1 = dz \n",
        "        \n",
        "        self.grad = grad # dl / da2\n",
        "        # dl/da2 * da2/dz2(시그모이드 역전파(self.layers[1].a))\n",
        "        # dl/da2는 axis = 0으로 sum 해주어서 브로드캐스팅을 사용하기 위해 일반 곱을 사용\n",
        "        self.dz2 = self.grad * self.layers[1].activation.backward(self.layers[1].a)# dl/da2 * da2/dz2 (4, 1)\n",
        "        self.db2 = 1./ 4. * np.sum(self.dz2, axis = 0, keepdims= True)# (dz2/db2)1 * dl/dz2 덧셈 노드라 그냥 전달(1, 1)\n",
        "        self.dw2 = self.layers[0].a.T @ self.dz2 # a1으로 w2 값 구하고  dz2/dw2 * da2/dz2 (8, 4) (4, 1) = (8, 1)\n",
        "        \n",
        "        \n",
        "        #print(f'dz2{self.dz2.shape}, w2{self.layers[1].weights.T.shape}, z1{self.layers[0].activation.backward(self.a1).shape}')\n",
        "        # dl/dz2 * dz2/da1 * da1/dz1   \n",
        "        self.dz1 =  self.dz2 @ self.layers[1].weights.T * self.layers[0].activation.backward(self.a1)  #(4, 1) (1, 8) (4, 8)\n",
        "        self.dw1 = self.input.T @ self.dz1 # (2, 4) @ (4, 8) = (2, 8)\n",
        "        self.db1 = 1./4.*np.sum(self.dz1, axis = 0, keepdims= True)\n",
        "       \n",
        "        return [[self.db1, self.db2], [self.dw1, self.dw2]]\n",
        "        \n",
        "\n",
        "    def update(self):\n",
        "        # 한번만 되고 계속 같은값이 나온 이유는 갱신이 안되서\n",
        "\n",
        "        self.layers[0].weights -= self.dw1 * self.lr\n",
        "        self.layers[1].weights -= self.dw2 * self.lr\n",
        "        self.layers[0].bias -= self.db1 + self.lr\n",
        "        self.layers[1].bias -= self.db2 * self.lr\n",
        "\n",
        "        return [self.layers[0].weights, self.layers[1].weights, self.layers[0].bias, self.layers[1].bias]\n",
        "        \n",
        "\n",
        "\n",
        "    def train_once(self, x, y):\n",
        "        self.y_pred = self.forward(x)\n",
        "        loss = self.loss.backward(y, self.y_pred)\n",
        "        self.backward(loss)\n",
        "        self.update() \n",
        "    \n",
        "    def train(self, x, y, batch_size = 1, epochs = 1, interval = 1):\n",
        "        self.loss_trace = []\n",
        "        for i in range(epochs):\n",
        "            self.train_once(x, y)\n",
        "            if i % interval == 0:\n",
        "                self.loss_trace.append(self.loss.forward(y, self.y_pred).reshape(1))\n",
        "                print(self.loss.forward(y, self.y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_xor_dataset():\n",
        "    return np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 0]).reshape(4, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzu09zB5m-4J"
      },
      "source": [
        "np.random.seed(seed=100)\n",
        "model = Model()\n",
        "model.add(Perceptron(np.random.normal(size=(2, 8)), np.random.normal(size=(1, 8)), activation=Sigmoid()))\n",
        "model.add(Perceptron(np.random.normal(size=(8, 1)), np.random.normal(size=(1, 1)), activation=Sigmoid()))\n",
        "model.set_loss(MSE())\n",
        "model.lr = 0.1\n",
        "x_data, y_data = create_xor_dataset()# x = (4, 2)\n",
        "model.train(x_data, y_data, batch_size=1, epochs=1000, interval=10)\n",
        "z =  model.forward(x_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMpAWkNXAikz"
      },
      "source": [
        "# 학습 2차 시도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULTUzJWF7GSz"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):# 모든 함수들이 공통으로 입출력을 조절\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        \n",
        "    \n",
        "    def forward(self, x):# 모든 함수들이 공통으로 사용하는 순전파 \n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self, grad = 1):\n",
        "        if self.input is None or self.output is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights  # (in, out)\n",
        "        self.bias = bias  # (1, out)\n",
        "        self.activation = activation\n",
        "        # print(self.weights,self.bias)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        y = np.matmul(x, self.weights) + self.bias  # (N, out)\n",
        "        if self.activation is None:\n",
        "            return y\n",
        "        y = self.activation.forward(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        if self.activation is None:\n",
        "            grad_w = (\n",
        "                np.matmul(np.transpose(self.input), grad) / self.input.shape[0]\n",
        "            )  # (in, out)\n",
        "            grad_b = np.mean(grad, axis=0, keepdims=True)  # (1, out)\n",
        "            return grad_w, grad_b\n",
        "        grad_a = self.activation.backward(grad)  # (N, out)\n",
        "        grad_w = (\n",
        "            np.matmul(np.transpose(self.input), grad_a) / self.input.shape[0]\n",
        "        )  # (in, out)\n",
        "        grad_b = np.mean(grad_a, axis=0, keepdims=True)  # (1, out)\n",
        "        grad_next = np.matmul(grad_a, np.transpose(self.weights))  # (N, in)\n",
        "        return (grad_w, grad_b), grad_next\n",
        "\n",
        "    def update(self, grads):\n",
        "        grad_w, grad_b = grads\n",
        "        self.weights -= grad_w\n",
        "        self.bias -= grad_b\n",
        "\n",
        "class Loss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "   \n",
        "    def _forward(self, y_true, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, y_ture, y_pred):\n",
        "        raise NotImplementedError\n",
        "\n",
        "        \n",
        "\n",
        "class MSE(Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_true, y_pred):# forward를 상속받아서 고쳐쓴 것\n",
        "        self.mse = 0.5 * np.mean(np.square(y_pred - y_true) , axis = 0, keepdims = True)# 1랭크가 되지 않도록 차원 유지!\n",
        "        return self.mse\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        self.grad = np.mean(y_pred - y_true, axis = 0, keepdims=True)# 평균, 차원 유지\n",
        "        return self.grad\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def _forward(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        return grad * self.output * (1 - self.output)  # (N, out)\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self, lr = 0.001):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.loss = None #loss값 초기화 이전 값이 있더라도 model()클래스 선언시 초기화됨!\n",
        "        self.lr = lr\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        print(self.layers)\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "        \n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        return self.loss.forward(y, self.forward(x))\n",
        "\n",
        "    \n",
        "    def _backward(self, grad):\n",
        "        grads = []\n",
        "        layer_reversed = self.layers.copy()\n",
        "        layer_reversed.reverse()\n",
        "        for layer in layer_reversed:\n",
        "            layer_grad, next_grad = layer.backward(grad)\n",
        "            grad = next_grad\n",
        "            grads.append((layer, layer_grad))\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def update(self, grads):\n",
        "        for idx, grad_layer in enumerate(grads):\n",
        "            layer, grad = grad_layer\n",
        "            grad_w, grad_b = grad\n",
        "            update_grad = (grad_w * self.lr, grad_b * self.lr)\n",
        "            layer.update(update_grad)\n",
        "\n",
        "\n",
        "    def train_once(self, x, y):\n",
        "        logit = self.forward(x)\n",
        "        dout = self.loss.backward(y, logit)\n",
        "        grads = self.backward(dout)\n",
        "        self.update(grads)\n",
        "\n",
        "\n",
        "    def train(self, x, y, batch_size=1, epochs=100000, interval=1000):\n",
        "        def batch_generator(x, y, batch_size):\n",
        "            i = 0\n",
        "            while i < len(x):\n",
        "                batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                yield batch, y_batch\n",
        "                i += batch_size\n",
        "\n",
        "\n",
        "        epoch = 0\n",
        "        self.loss_trace = []\n",
        "        while epoch < epochs:\n",
        "            gen = batch_generator(x, y, batch_size)\n",
        "            try:\n",
        "                while 1:\n",
        "                    batch, y_batch = next(gen)\n",
        "                    logit_batch = self.forward(batch)\n",
        "                    dout = self.loss.backward(y_batch, logit_batch)\n",
        "                    grads = self.backward(dout)\n",
        "                    self.update(grads)\n",
        "            except StopIteration:\n",
        "                if not (epoch % interval):\n",
        "                    print(np.mean(self.evaluate(x, y)))\n",
        "                    self.loss_trace.append(np.mean(self.evaluate(x, y)))\n",
        "                epoch += 1\n",
        "                # print(epoch)\n",
        "\n",
        "\n",
        "\n",
        "def create_xor_dataset():\n",
        "    return np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 0]).reshape(4, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "TLGSFzU3igoA",
        "outputId": "8762b810-46e3-4dce-fea1-d34990e6fc74"
      },
      "source": [
        ">>> model = Model()\n",
        ">>> model.add(Perceptron(np.random.normal(size=(3, 5)), np.random.normal(size=(1, 5)), activation=Sigmoid()))\n",
        ">>> model.add(Perceptron(np.random.normal(size=(5, 3)), np.random.normal(size=(1, 3)), activation=Sigmoid()))\n",
        ">>> model.add(Perceptron(np.random.normal(size=(3, 3)), np.random.normal(size=(1, 3)), activation=Sigmoid()))\n",
        ">>> model.add(Perceptron(np.random.normal(size=(3, 1)), np.random.normal(size=(1, 1)), activation=MSE()))\n",
        ">>> model.train(x_train, y_train, batch_size=20, epochs=1000)\n",
        ">>> model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4085cd3ea851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, batch_size, epochs, interval)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                     \u001b[0mlogit_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# 모든 함수들이 공통으로 사용하는 순전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# 모든 함수들이 공통으로 사용하는 순전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'y_pred'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-c-2UFk_zIj"
      },
      "source": [
        "3차 시도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceMIlSt-9IWr"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.lib.function_base import bartlett\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.inputs = x\n",
        "        self.outputs = self._forward(x)\n",
        "        return self.outputs\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self, grad=1.0):\n",
        "        if self.inputs is None or self.outputs is None:\n",
        "            return None\n",
        "        return self._backward(grad)\n",
        "\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def update(self, grads):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        return grad * self.outputs * (1 - self.outputs)  # (N, out)\n",
        "\n",
        "\n",
        "\n",
        "class Perceptron(Layer):\n",
        "    def __init__(self, weights, bias, activation=None):\n",
        "        super().__init__()\n",
        "        self.weights = weights  # (in, out)\n",
        "        self.bias = bias  # (1, out)\n",
        "        self.activation = activation\n",
        "        # print(self.weights,self.bias)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        y = np.matmul(x, self.weights) + self.bias  # (N, out)\n",
        "        if self.activation is None:\n",
        "            return y\n",
        "        y = self.activation.forward(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        if self.activation is None:\n",
        "            grad_w = (\n",
        "                np.matmul(np.transpose(self.inputs), grad) / self.inputs.shape[0]\n",
        "            )  # (in, out)\n",
        "            grad_b = np.mean(grad, axis=0, keepdims=True)  # (1, out)\n",
        "            return grad_w, grad_b\n",
        "        grad_a = self.activation.backward(grad)  # (N, out)\n",
        "        grad_w = (\n",
        "            np.matmul(np.transpose(self.inputs), grad_a) / self.inputs.shape[0]\n",
        "        )  # (in, out)\n",
        "        grad_b = np.mean(grad_a, axis=0, keepdims=True)  # (1, out)\n",
        "        grad_next = np.matmul(grad_a, np.transpose(self.weights))  # (N, in)\n",
        "        return (grad_w, grad_b), grad_next\n",
        "\n",
        "\n",
        "    def update(self, grads):\n",
        "        grad_w, grad_b = grads\n",
        "        self.weights -= grad_w\n",
        "        self.bias -= grad_b\n",
        "\n",
        "\n",
        "\n",
        "class StepFunction(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        y = np.where(x > 0, True, False)\n",
        "        return y.astype(np.int)\n",
        "\n",
        "\n",
        "\n",
        "W = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "b = np.array([[5.0, 6.0]])\n",
        "\n",
        "\n",
        "\n",
        "X_train = np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
        "p = Perceptron(W, b)\n",
        "\n",
        "\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.lr = lr\n",
        "\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def _forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def set_loss(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        if self.loss is None:\n",
        "            return None\n",
        "        return self.loss.forward(y, self.forward(x))\n",
        "\n",
        "\n",
        "    def _backward(self, grad):\n",
        "        grads = []\n",
        "        layer_reversed = self.layers.copy()\n",
        "        layer_reversed.reverse()\n",
        "        for layer in layer_reversed:\n",
        "            layer_grad, next_grad = layer.backward(grad)\n",
        "            grad = next_grad\n",
        "            grads.append((layer, layer_grad))\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def update(self, grads):\n",
        "        for idx, grad_layer in enumerate(grads):\n",
        "            layer, grad = grad_layer\n",
        "            grad_w, grad_b = grad\n",
        "            update_grad = (grad_w * self.lr, grad_b * self.lr)\n",
        "            layer.update(update_grad)\n",
        "\n",
        "\n",
        "    def train_once(self, x, y):\n",
        "        logit = self.forward(x)\n",
        "        dout = self.loss.backward(y, logit)\n",
        "        grads = self.backward(dout)\n",
        "        self.update(grads)\n",
        "\n",
        "\n",
        "    def train(self, x, y, batch_size=1, epochs=100000, interval=1000):\n",
        "        def batch_generator(x, y, batch_size):\n",
        "            i = 0\n",
        "            while i < len(x):\n",
        "                batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                yield batch, y_batch\n",
        "                i += batch_size\n",
        "\n",
        "\n",
        "        epoch = 0\n",
        "        while epoch < epochs:\n",
        "            gen = batch_generator(x, y, batch_size)\n",
        "            try:\n",
        "                while 1:\n",
        "                    batch, y_batch = next(gen)\n",
        "                    logit_batch = self.forward(batch)\n",
        "                    dout = self.loss.backward(y_batch, logit_batch)\n",
        "                    grads = self.backward(dout)\n",
        "                    self.update(grads)\n",
        "            except StopIteration:\n",
        "                if not (epoch % interval):\n",
        "                    print(np.mean(self.evaluate(x, y)))\n",
        "                epoch += 1\n",
        "                # print(epoch)\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "def create_xor_dataset():\n",
        "    return np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h_p3RnNz7mOk",
        "outputId": "09fa1604-2d74-4ace-d548-af906ca99771"
      },
      "source": [
        "np.random.seed(seed=100)\n",
        "model = Model()\n",
        "model.add(Perceptron(np.random.normal(size=(2, 8)), np.random.normal(size=(1, 8)), activation=Sigmoid()))\n",
        "model.add(Perceptron(np.random.normal(size=(8, 1)), np.random.normal(size=(1, 1)), activation=Sigmoid()))\n",
        "model.set_loss(MSE())\n",
        "model.lr = 0.1\n",
        "x_data, y_data = create_xor_dataset()# x = (4, 2)\n",
        "model.train(x_data, y_data, batch_size=1, epochs=100000, interval=100)\n",
        "z =  model.forward(x_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12702695724262836\n",
            "0.1227572217886189\n",
            "0.12122594784405295\n",
            "0.11957729964027171\n",
            "0.11764914991339444\n",
            "0.11528578043703269\n",
            "0.11231120614473944\n",
            "0.10850888888669914\n",
            "0.10361179263356814\n",
            "0.09732512381693842\n",
            "0.08942064523097008\n",
            "0.07991949720899465\n",
            "0.06927406686423067\n",
            "0.05834832621650844\n",
            "0.0481065620247907\n",
            "0.039230149965129565\n",
            "0.03196270569579714\n",
            "0.02621184098277541\n",
            "0.021730178935906858\n",
            "0.01824565489108999\n",
            "0.015521136647587221\n",
            "0.01336962639051943\n",
            "0.011650291127653561\n",
            "0.010259064792651791\n",
            "0.009119399681135894\n",
            "0.008174800285321363\n",
            "0.007383250396701379\n",
            "0.006713192756118619\n",
            "0.006140667007436379\n",
            "0.005647277139643201\n",
            "0.005218742849426684\n",
            "0.004843859924274323\n",
            "0.004513747630559785\n",
            "0.004221298604027396\n",
            "0.003960772718151457\n",
            "0.0037274942263593727\n",
            "0.003517623679730228\n",
            "0.0033279845067507936\n",
            "0.0031559299347649183\n",
            "0.002999239964187574\n",
            "0.0028560409349836953\n",
            "0.0027247422264478098\n",
            "0.0026039860601911454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-96d1115949f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_xor_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# x = (4, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, batch_size, epochs, interval)\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogit_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlayer_reversed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_reversed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mlayer_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         grad_w = (\n\u001b[1;32m     53\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-aedd26c6f916>\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTUETFP-7FfX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il6wxl0fisLc",
        "outputId": "06d2c6de-9ed6-49fb-c7c8-04f7f9170e48"
      },
      "source": [
        "model.forward(x_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00652213],\n",
              "       [0.99176725],\n",
              "       [0.98964205],\n",
              "       [0.01137146]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "L1nuL1oo1-ca",
        "outputId": "804c7020-8741-4550-b7eb-5459e075e72a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot learning curve (with costs)\n",
        "plt.plot(model.loss_trace)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFrQFkmX6RCb",
        "outputId": "e2f9b50d-a4c5-44af-8ea7-992abcd17e3d"
      },
      "source": [
        "s = np.array([[0.5], [0.5] , [0.5], [0.5]])\n",
        "y = np.array([[0], [1] , [1], [0]])\n",
        "delta=0.001\n",
        "\n",
        "z = -np.sum(y*np.log(s+delta)+(1-y)*np.log(1-s+delta))\n",
        "\n",
        "print(z)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.764596711589089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npbj5KeNE90L"
      },
      "source": [
        "##다른답"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AeZFR-PU_7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7828df8b-5ce5-4514-a550-55f06ff66b86"
      },
      "source": [
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "    \n",
        "    def forward(self,x):\n",
        "        self.input = x\n",
        "        self.output = self._forward(x)\n",
        "        return self.output\n",
        "    \n",
        "    def _forward(self,x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def backward(self,x=None):\n",
        "        return self._backward(x)\n",
        "\n",
        "\n",
        "    def _backward(self,x=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Perceptron(object):\n",
        "    def __init__(self,weight,bias,activation=None):\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "\n",
        "\n",
        "class MSE(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self,s,y):\n",
        "        m = len(x)\n",
        "        return (1/2)*np.sum((s-y)**2)\n",
        "\n",
        "\n",
        "    def backward(self,s,y):\n",
        "        m = len(x)\n",
        "        return (s-y)\n",
        "\n",
        "\n",
        "class CRE(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self,s,y):\n",
        "        delta = 0.001\n",
        "        return -np.sum(y*np.log(s+delta)+(1-y)*np.log(1-s+delta))\n",
        "    def backward(self,s,y):\n",
        "        return (s-y)\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def _forward(self,x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "    def _backward(self,x):\n",
        "        s = self._forward(x)\n",
        "        return (1-s) * s\n",
        "\n",
        "\n",
        "class Model(Layer):\n",
        "    def __init__(self,x,y,ep=10000,lr=0.01):\n",
        "        super().__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.ep = ep\n",
        "        self.lr = lr\n",
        "        self.layers = []\n",
        "\n",
        "\n",
        "    def set_node(self):\n",
        "        self.W1 = self.layers[0].weight\n",
        "        self.b1 = self.layers[0].bias\n",
        "        self.layers[0].weight = self.W1\n",
        "        self.layers[0].bias = self.b1\n",
        "        self.z1 = np.dot(self.x,self.W1)+self.b1\n",
        "        self.s1 = self.layers[0].activation.forward(self.z1)\n",
        "\n",
        "\n",
        "        self.W2 = self.layers[1].weight\n",
        "        self.b2 = self.layers[1].bias\n",
        "        self.layers[1].weight = self.W2\n",
        "        self.layers[1].bias = self.b2\n",
        "        self.z2 = np.dot(self.s1,self.W2)+self.b2\n",
        "        self.s2 = self.layers[1].activation.forward(self.z2)\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        # input_data = self.x\n",
        "        # for i in range(len(self.layers)):\n",
        "        #     input_data = np.dot(input_data,self.layers[i].weight)+self.layers[i].bias\n",
        "        #     activation = self.layers[i].activation.forward(input_data)\n",
        "        #     input_data = activation\n",
        "        # self.cost = self.loss_func.forward(input_data,self.y)        \n",
        "        self.W1 = self.layers[0].weight\n",
        "        self.b1 = self.layers[0].bias\n",
        "        self.layers[0].weight = self.W1\n",
        "        self.layers[0].bias = self.b1\n",
        "        self.z1 = np.dot(self.x,self.W1)+self.b1\n",
        "        self.s1 = self.layers[0].activation.forward(self.z1)\n",
        "\n",
        "\n",
        "        self.W2 = self.layers[1].weight\n",
        "        self.b2 = self.layers[1].bias\n",
        "        self.layers[1].weight = self.W2\n",
        "        self.layers[1].bias = self.b2\n",
        "        self.z2 = np.dot(self.s1,self.W2)+self.b2\n",
        "        self.s2 = self.layers[1].activation.forward(self.z2)\n",
        "        return self.s2\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        yhat = self.forward()\n",
        "        #오차함수 미분\n",
        "        dL = self.loss_func.backward(yhat,self.y)\n",
        "        #output layer 활성화 함수 미분\n",
        "        dS2 = self.layers[1].activation.backward(self.z2)\n",
        "        loss2 = dL*dS2\n",
        "        #output layer W 미분\n",
        "        self.dw2 = np.dot(self.s1.T,loss2)\n",
        "        \n",
        "        #output layer b 미분\n",
        "        self.db2 = np.sum(loss2,axis=0)\n",
        "\n",
        "\n",
        "        #hidden layer 활성화함수 미분\n",
        "        loss1 = np.dot(loss2,self.W2.T)*self.layers[0].activation.backward(self.z1)\n",
        "        self.dw1 = np.dot(self.x.T,loss1)\n",
        "        self.db1 = np.sum(loss1,axis=0)\n",
        "\n",
        "\n",
        "    def add(self,layer):\n",
        "        self.layers.append(layer)\n",
        "    \n",
        "    def set_loss(self,loss_func):\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        self.W2 -= self.lr*self.dw2\n",
        "        self.b2 -= self.lr*self.db2\n",
        "\n",
        "\n",
        "        self.W1 -= self.lr*self.dw1\n",
        "        self.b1 -= self.lr*self.db1\n",
        "        self.set_node()\n",
        "    def train(self):\n",
        "        self.forward()\n",
        "        self.backward()\n",
        "        self.update()\n",
        "        self.set_node()\n",
        "    def get_cost(self):\n",
        "        return self.loss_func.forward(self.s2,self.y)\n",
        "\n",
        "\n",
        "x = np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y = np.array([[0.],[1.],[1.],[0.]])\n",
        "\n",
        "\n",
        "model = Model(x,y,lr=0.1)\n",
        "model.add(Perceptron(np.random.randn(2,8),np.random.randn(8),Sigmoid()))\n",
        "model.add(Perceptron(np.random.randn(8,1),np.random.randn(1),Sigmoid()))\n",
        "model.set_loss(CRE())\n",
        "model.set_node()\n",
        "\n",
        "\n",
        "\n",
        "for i in range(100000):\n",
        "    model.backward()\n",
        "    model.update()\n",
        "    if i%1000==0:\n",
        "        print(model.get_cost())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.3190048160352226\n",
            "1.673269726277934\n",
            "0.6331913189161782\n",
            "0.3919054272620028\n",
            "0.2955066821248415\n",
            "0.24283667508616924\n",
            "0.20912154943104855\n",
            "0.18541588065893655\n",
            "0.16768585226573468\n",
            "0.15383381729591142\n",
            "0.1426551875007539\n",
            "0.13340576754744946\n",
            "0.12559926250287332\n",
            "0.11890346645732063\n",
            "0.11308310508653123\n",
            "0.10796650413340533\n",
            "0.10342521064318928\n",
            "0.09936102977865667\n",
            "0.09569749615206875\n",
            "0.09237409749784635\n",
            "0.08934226242744218\n",
            "0.08656251103499865\n",
            "0.08400239121795694\n",
            "0.08163495766757259\n",
            "0.07943763308823662\n",
            "0.07739134342861799\n",
            "0.07547985270279417\n",
            "0.07368924531353482\n",
            "0.07200751883379716\n",
            "0.0704242605135785\n",
            "0.06893038795981495\n",
            "0.06751793951111704\n",
            "0.06617990346342642\n",
            "0.06491007793831151\n",
            "0.06370295511932425\n",
            "0.06255362501586331\n",
            "0.061457694988195546\n",
            "0.06041122207954372\n",
            "0.05941065582076825\n",
            "0.058452789649743514\n",
            "0.057534719456939054\n",
            "0.0566538080571966\n",
            "0.05580765461446535\n",
            "0.054994068225783316\n",
            "0.0542110450137047\n",
            "0.053456748190892606\n",
            "0.052729490652805605\n",
            "0.05202771972908672\n",
            "0.05135000378507365\n",
            "0.05069502041451768\n",
            "0.05006154600546969\n",
            "0.049448446494974194\n",
            "0.048854669156145863\n",
            "0.04827923528443262\n",
            "0.04772123366927545\n",
            "0.04717981475363568\n",
            "0.04665418539756039\n",
            "0.04614360417348835\n",
            "0.045647377130801164\n",
            "0.04516485397541276\n",
            "0.04469542461730279\n",
            "0.04423851604491954\n",
            "0.043793589490595565\n",
            "0.04336013785555307\n",
            "0.04293768336693525\n",
            "0.042525775442608625\n",
            "0.042123988742357406\n",
            "0.04173192138658196\n",
            "0.04134919332579481\n",
            "0.04097544484608274\n",
            "0.04061033519736752\n",
            "0.040253541332749596\n",
            "0.03990475674847096\n",
            "0.039563690415174546\n",
            "0.03923006579209738\n",
            "0.038903619916724634\n",
            "0.038584102563175626\n",
            "0.03827127546329499\n",
            "0.037964911585008546\n",
            "0.03766479446304656\n",
            "0.03737071757761103\n",
            "0.03708248377698842\n",
            "0.03679990474049359\n",
            "0.036522800478453965\n",
            "0.03625099886627404\n",
            "0.0359843352098585\n",
            "0.03572265183994852\n",
            "0.0354657977331203\n",
            "0.03521362815740975\n",
            "0.03496600434069497\n",
            "0.03472279316013396\n",
            "0.03448386685110158\n",
            "0.034249102734190455\n",
            "0.03401838295897883\n",
            "0.03379159426335744\n",
            "0.033568627747314055\n",
            "0.03334937866016506\n",
            "0.0331337462002988\n",
            "0.032921633326574544\n",
            "0.0327129465805827\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}